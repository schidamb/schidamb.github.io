---
---

@string{acm = {Association for Computing Machinery,}}

@article{AnnotateXR,
  author = {Chidambaram*, Subramanian and Jain*, Rahul  and Reddy, Sai Swarup and Unmesh, Asim and Ramani, Karthik},
  title = "{AnnotateXR: An Extended Reality Workflow for Automating Data Annotation to Support Computer Vision Applications}",
  journal = {Journal of Computing and Information Science in Engineering},
  volume = {24},
  number = {3},
  pages = {031005},
  year = {2024},
  month = {08},
  abstract = "{Computer Vision (CV) algorithms require large annotated datasets that are often labor-intensive and expensive to create. We propose AnnotateXR, an extended reality (XR) workflow to collect various high fidelity data and auto-annotate it in a single demonstration. AnnotateXR allows users to align virtual models over physical objects, tracked with 6DoF sensors. AnnotateXR utilizes a hand tracking capable XR HMD coupled with 6DoF information and collision detection to enable algorithmic segmentation of different actions in videos through its digital twin. The virtual-physical mapping provides a tight bounding volume to generate semantic segmentation masks for the captured image data. Alongside supporting object and action segmentation, we also support other dimensions of annotation required by modern CV, such as Human-Object, Object-Object, and rich 3D recordings, all with a single demonstration. Our user study shows AnnotateXR produced over 112,000 annotated data points in 67 minutes.}",
  issn = {1530-9827},
  doi = {10.1115/1.4062970},
  url = {https://doi.org/10.1115/1.4062970},
  eprint = {https://asmedigitalcollection.asme.org/computingengineering/article-pdf/24/3/031005/7050188/jcise\_24\_3\_031005.pdf},
  pdf={AnnotateXR.pdf},
  video={https://www.youtube.com/watch?v=lORFgXiVIKE},
  preview={AnnotateXR_Preview.jpg},
  selected={true}
}

@inproceedings{EditAR,
  author={Chidambaram, Subramanian and Reddy, Sai Swarup and Rumple, Matthew and Ipsita, Ananya and Villanueva, Ana and Redick, Thomas and Stuerzlinger, Wolfgang and Ramani, Karthik},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={EditAR: A Digital Twin Authoring Environment for Creation of AR/VR and Video Instructions from a Single Demonstration}, 
  abstract = {Augmented/Virtual reality and video-based media play a vital role in the digital learning revolution to train novices in spatial tasks. However, creating content for these different media requires expertise in several fields. We present EditAR, a unified authoring, and editing environment to create content for AR, VR, and video based on a single demonstration. EditAR captures the user’s interaction within an environment and creates a digital twin, enabling users without programming backgrounds to develop content. We conducted formative interviews with both subject and media experts to design the system. The prototype was developed and reviewed by experts. We also performed a user study comparing traditional video creation with 2D video creation from 3D recordings, via a 3D editor, which uses freehand interaction for in-headset editing. Users took 5 times less time to record instructions and preferred EditAR, along with giving significantly higher usability scores.},
  year={2022},
  volume={},
  number={},
  pages={326-335},
  keywords={Three-dimensional displays;Prototypes;Media;Digital twins;Recording;Usability;Task analysis;Augmented Reality;Virtual Reality;Authoring;Video},
  pdf={EditAR.pdf},
  video={https://www.youtube.com/watch?v=lvf0ga4Dztc&t=1s&ab_channel=CdesignLab},
  doi={10.1109/ISMAR55827.2022.00048},
  preview={EditAR.png},
  selected={true}
  }

@inproceedings{ProcessAR,
author = {Chidambaram, Subramanian and Huang, Hank and He, Fengming and Qian, Xun and Villanueva, Ana M and Redick, Thomas S and Stuerzlinger, Wolfgang and Ramani, Karthik},
title = {ProcessAR: An augmented reality-based tool to create in-situ procedural 2D/3D AR Instructions},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461778.3462126},
doi = {10.1145/3461778.3462126},
abstract = {Augmented reality (AR) is an efficient form of delivering spatial information and has great potential for training workers. However, AR is still not widely used for such scenarios due to the technical skills and expertise required to create interactive AR content. We developed ProcessAR, an AR-based system to develop 2D/3D content that captures subject matter expert’s (SMEs) environment-object interactions in situ. The design space for ProcessAR was identified from formative interviews with AR programming experts and SMEs, alongside a comparative design study with SMEs and novice users. To enable smooth workflows, ProcessAR locates and identifies different tools/objects through computer vision within the workspace when the author looks at them. We explored additional features such as embedding 2D videos with detected objects and user-adaptive triggers. A final user evaluation comparing ProcessAR and a baseline AR authoring environment showed that, according to our qualitative questionnaire, users preferred ProcessAR.},
booktitle = {Proceedings of the 2021 ACM Designing Interactive Systems Conference},
pages = {234-249},
numpages = {16},
keywords = {Augmented Reality, Authoring, Computer Vision, Tutorials},
pdf={ProcessAR.pdf},
video={https://www.youtube.com/watch?v=8VmminfizxU&ab_channel=CdesignLab},
preview={ProcessAR.png},
location = {Virtual Event, USA},
series = {DIS '21},
selected={true}
}

@inproceedings{Shape_Structuralizer,
author = {Chidambaram*, Subramanian and Zhang*, Yunbo and Sundararajan, Venkatraghavan and Elmqvist, Niklas and Ramani, Karthik},
title = {Shape Structuralizer: Design, Fabrication, and User-driven Iterative Refinement of 3D Mesh Models},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300893},
doi = {10.1145/3290605.3300893},
abstract = {Current Computer-Aided Design (CAD) tools lack proper support for guiding novice users towards designs ready for fabrication. We propose Shape Structuralizer (SS), an interactive design support system that repurposes surface models into structural constructions using rods and custom 3D-printed joints. Shape Structuralizer embeds a recommendation system that computationally supports the user during design ideation by providing design suggestions on local refinements of the design. This strategy enables novice users to choose designs that both satisfy stress constraints as well as their personal design intent. The interactive guidance enables users to repurpose existing surface mesh models, analyze them in-situ for stress and displacement constraints, add movable joints to increase functionality, and attach a customized appearance. This also empowers novices to fabricate even complex constructs while ensuring structural soundness. We validate the Shape Structuralizer tool with a qualitative user study where we observed that even novice users were able to generate a large number of structurally safe designs for fabrication.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1-12},
numpages = {12},
keywords = {fabrication, design recommendation, cad, 3d modeling},
pdf={Shape_Structuralizer.pdf},
video={https://www.youtube.com/watch?v=G1dSmxgI7wA&ab_channel=CdesignLab},
location = {Glasgow, Scotland Uk},
series = {CHI '19},
preview={Shape_Structuralizer.jpg},
selected={false}
}

@inproceedings{iSoft,
author = {Yoon, Sang Ho and Huo, Ke and Zhang, Yunbo and Chen, Guiming and Paredes, Luis and Chidambaram, Subramanian and Ramani, Karthik},
title = {iSoft: A Customizable Soft Sensor with Real-time Continuous Contact and Stretching Sensing},
year = {2017},
isbn = {9781450349819},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126594.3126654},
doi = {10.1145/3126594.3126654},
abstract = {We present iSoft, a single volume soft sensor capable of sensing real-time continuous contact and unidirectional stretching. We propose a low-cost and an easy way to fabricate such piezoresistive elastomer-based soft sensors for instant interactions. We employ an electrical impedance tomography (EIT) technique to estimate changes of resistance distribution on the sensor caused by fingertip contact. To compensate for the rebound elasticity of the elastomer and achieve real-time continuous contact sensing, we apply a dynamic baseline update for EIT. The baseline updates are triggered by fingertip contact and movement detections. Further, we support unidirectional stretching sensing using a model-based approach which works separately with continuous contact sensing. We also provide a software toolkit for users to design and deploy personalized interfaces with customized sensors. Through a series of experiments and evaluations, we validate the performance of contact and stretching sensing. Through example applications, we show the variety of examples enabled by iSoft.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
pages = {665-678},
numpages = {14},
keywords = {customization, input device, sensing technique, soft sensor, wearables},
location = {Qu\'{e}bec City, QC, Canada},
series = {UIST '17},
pdf={iSoft.pdf},
video={https://www.youtube.com/watch?v=JVaYEl9nbME&ab_channel=CdesignLab},
preview={iSoft.jpg}
}

@inproceedings{FabHandWear,
author = {Paredes, Luis and Reddy, Sai Swarup and Chidambaram, Subramanian and Vagholkar, Devashri and Zhang, Yunbo and Benes, Bedrich and Ramani, Karthik},
title = {FabHandWear: An End-to-End Pipeline from Design to Fabrication of Customized Functional Hand Wearables},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
url = {https://doi.org/10.1145/3463518},
doi = {10.1145/3463518},
abstract = {Current hand wearables have limited customizability, they are loose-fit to an individual's hand and lack comfort. The main barrier in customizing hand wearables is the geometric complexity and size variation in hands. Moreover, there are different functions that the users can be looking for; some may only want to detect hand's motion or orientation; others may be interested in tracking their vital signs. Current wearables usually fit multiple functions and are designed for a universal user with none or limited customization. There are no specialized tools that facilitate the creation of customized hand wearables for varying hand sizes and provide different functionalities. We envision an emerging generation of customizable hand wearables that supports hand differences and promotes hand exploration with additional functionality. We introduce FabHandWear, a novel system that allows end-to-end design and fabrication of customized functional self-contained hand wearables. FabHandWear is designed to work with off-the-shelf electronics, with the ability to connect them automatically and generate a printable pattern for fabrication. We validate our system by using illustrative applications, a durability test, and an empirical user evaluation. Overall, FabHandWear offers the freedom to create customized, functional, and manufacturable hand wearables.},
journal = {Proceedings of ACM Interactions Mobile Wearable Ubiquitous Technology},
month = {jun},
articleno = {76},
numpages = {22},
keywords = {textiles, screen print, interface, inks, hand, fabrication, electronics, customization, Wearables, 3D design},
pdf={FabHandWear.pdf},
video={https://www.youtube.com/watch?v=0B0PWLcIURE&ab_channel=CdesignLab},
preview={FabHandWear.jpg}
}

@inproceedings{VRfromX,
author = {Ipsita, Ananya and Li, Hao and Duan, Runlin and Cao, Yuanzhi and Chidambaram, Subramanian and Liu, Min and Ramani, Karthik},
title = {VRFromX: From Scanned Reality to Interactive Virtual Experience with Human-in-the-Loop},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451747},
doi = {10.1145/3411763.3451747},
abstract = {There is an increasing trend of Virtual-Reality (VR) applications found in education, entertainment, and industry. Many of them utilize real world tools, environments, and interactions as bases for creation. However, creating such applications is tedious, fragmented, and involves expertise in authoring VR using programming and 3D-modelling softwares. This hinders VR adoption by decoupling subject matter experts from the actual process of authoring while increasing cost and time. We present VRFromX, an in-situ Do-It-Yourself (DIY) platform for content creation in VR that allows users to create interactive virtual experiences. Using our system, users can select region(s) of interest (ROI) in scanned point cloud or sketch in mid-air using a brush tool to retrieve virtual models and then attach behavioral properties to them. We ran an exploratory study to evaluate usability of VRFromX and the results demonstrate feasibility of the framework as an authoring tool. Finally, we implemented three possible use-cases to showcase potential applications.},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {289},
numpages = {7},
keywords = {Virtual Reality, Scene reconstruction, Scene manipulation, Point cloud interaction, Human-centered AI, Graphical user interface, Embodied interaction, Behavioral modelling},
location = {Yokohama, Japan},
series = {CHI EA '21},
pdf={VRFromX.pdf},
video={https://www.youtube.com/watch?v=27egu5VkL0M&ab_channel=CdesignLab},
preview={VRfromX.jpg}
}

@article{ColabAR,
author = {Villanueva, Ana and Zhu, Zhengzhe and Liu, Ziyi and Wang, Feiyang and Chidambaram, Subramanian and Ramani, Karthik},
title = {ColabAR: A Toolkit for Remote Collaboration in Tangible Augmented Reality Laboratories},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW1},
url = {https://doi.org/10.1145/3512928},
doi = {10.1145/3512928},
abstract = {Current times are accelerating new technologies to provide high-quality education for remote collaboration, as well as hands-on learning. This is particularly important in the case of laboratory-based classes, which play an essential role in STEM education. In this paper, we introduce ColabAR, a toolkit that uses physical proxies to manipulate virtual objects in Tangible Augmented Reality (TAR) laboratories. ColabAR introduces haptic-based customizable interaction techniques to promote remote collaboration between students. Our toolkit provides hardware and software that enable haptic feedback to improve user experience and promote collaboration during learning. Also, we present the architecture of our cloud platform for haptic interaction that supports information sharing between students in a TAR laboratory. We performed two user studies (N=40) to test the effect of our toolkit in enriching local and remote collaborative experiences. Finally, we demonstrated that our TAR laboratory enables students' performance (i.e., lab completion rate, lab scores) to be similar to their performance in an in-person laboratory.},
journal = {Proceedings ACM Human-Computer Interactions},
month = {apr},
articleno = {81},
numpages = {22},
keywords = {STEM, augmented reality, collaboration, distance, education, haptics, laboratory, learning, remote, tangibles},
pdf={VRFromX.pdf},
video={https://www.youtube.com/watch?v=27egu5VkL0M&ab_channel=CdesignLab},
preview={VRfromX.jpg}
}

@Article{mi12070784,
author = {Adam, Georges and Chidambaram, Subramanian and Reddy, Sai Swarup and Ramani, Karthik and Cappelleri, David J.},
title = {Towards a Comprehensive and Robust Micromanipulation System with Force-Sensing and VR Capabilities},
JOURNAL = {Micromachines},
VOLUME = {12},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {784},
URL = {https://www.mdpi.com/2072-666X/12/7/784},
PubMedID = {34209417},
ISSN = {2072-666X},
ABSTRACT = {In this modern world, with the increase of complexity of many technologies, especially in the micro and nanoscale, the field of robotic manipulation has tremendously grown. Microrobots and other complex microscale systems are often to laborious to fabricate using standard microfabrication techniques, therefore there is a trend towards fabricating them in parts then assembling them together, mainly using micromanipulation tools. Here, a comprehensive and robust micromanipulation platform is presented, in which four micromanipulators can be used simultaneously to perform complex tasks, providing the user with an intuitive environment. The system utilizes a vision-based force sensor to aid with manipulation tasks and it provides a safe environment for biomanipulation. Lastly, virtual reality (VR) was incorporated into the system, allowing the user to control the probes from a more intuitive standpoint and providing an immersive platform for the future of micromanipulation.},
DOI = {10.3390/mi12070784},
pdf={VR_Micromanipulators.pdf},
preview={VR_Micromanipulators.jpg}
}

@ARTICLE{Interacting_Objects,
  author={Unmesh, Asim and Jain, Rahul and Shi, Jingyu and Chaithanya Manam, V. K. and Chi, Hyung-Gun and Chidambaram, Subramanian and Quinn, Alexander and Ramani, Karthik},
  journal={IEEE Robotics and Automation Letters}, 
  title={Interacting Objects: A Dataset of Object-Object Interactions for Richer Dynamic Scene Representations}, 
  year={2024},
  volume={9},
  number={1},
  pages={451-458},
  abstract = {Dynamic environments in factories, surgical robotics, and warehouses increasingly involve humans, machines, robots, and various other objects such as tools, fixtures, conveyors, and assemblies. In these environments, numerous interactions occur not just between humans and objects but also between objects themselves. However, current scene-graph datasets predominantly focus on human-object interactions (HOI) and overlook objectobject interactions (OOIs) despite the necessity of OOIs in effectively representing dynamic environments. This oversight creates a significant gap in the coverage of interactive elements in dynamic scenes. We address this gap by proposing, to the best of our knowledge, the first dataset* annotating for OOI categories in dynamic scenes. To model OOIs, we establish a classification taxonomy for spatio-temporal interactions. We use our taxonomy to annotate OOIs in video clips of dynamic scenes. Then, we introduce a spatio-temporal OOI classification task which aims to identify interaction categories between two given objects in a video clip. Further, we benchmark our dataset for the spatio-temporal OOI classification task by adopting state-of-the-art approaches from related areas of Human-Object Interaction Classification, Visual Relationship Classification, and Scene-Graph Generation. Additionally, we utilize our dataset to examine the effectiveness of OOIandHOI-basedfeatures in the context of Action Recognition. Notably, our experimental results show that OOI-based features outperformHOI-based features for the task of Action Recognition.},
  keywords={Task analysis;Taxonomy;Annotations;Affordances;Visualization;Benchmark testing;Vehicle dynamics;Deep learning;machine vision;scene representation},
  doi={10.1109/LRA.2023.3332554},
  pdf={Interacting_Objects.pdf},
  preview={Interacting_Objects}
  }

@article{10.1115/1.4062970,
  author = {Ipsita, Ananya and Duan, Runlin and Li, Hao and Chidambaram, Subramanian and Cao, Yuanzhi and Liu, Min and Quinn, Alex and Ramani, Karthik},
  title = "{The Design of a Virtual Prototyping System for Authoring Interactive Virtual Reality Environments From Real-World Scans}",
  journal = {Journal of Computing and Information Science in Engineering},
  volume = {24},
  number = {3},
  pages = {031005},
  year = {2023},
  month = {10},
  abstract = "{Domain users (DUs) with a knowledge base in specialized fields are frequently excluded from authoring virtual reality (VR)-based applications in corresponding fields. This is largely due to the requirement of VR programming expertise needed to author these applications. To address this concern, we developed VRFromX, a system workflow design to make the virtual content creation process accessible to DUs irrespective of their programming skills and experience. VRFromX provides an in situ process of content creation in VR that (a) allows users to select regions of interest in scanned point clouds or sketch in mid-air using a brush tool to retrieve virtual models and (b) then attach behavioral properties to those objects. Using a welding use case, we performed a usability evaluation of VRFromX with 20 DUs from which 12 were novices in VR programming. Study results indicated positive user ratings for the system features with no significant differences across users with or without VR programming expertise. Based on the qualitative feedback, we also implemented two other use cases to demonstrate potential applications. We envision that the solution can facilitate the adoption of the immersive technology to create meaningful virtual environments.}",
  issn = {1530-9827},
  doi = {10.1115/1.4062970},
  url = {https://doi.org/10.1115/1.4062970},
  eprint = {https://asmedigitalcollection.asme.org/computingengineering/article-pdf/24/3/031005/7050188/jcise\_24\_3\_031005.pdf},
  pdf={JCISE_VRfromX.pdf},
  preview={VRfromX.jpg}
}

@misc{jain2024visualizing,
    title={Visualizing Causality in Mixed Reality for Manual Task Learning: An Exploratory Study}, 
    author={Rahul Jain* and Jingyu Shi* and Andrew Benton and Moiz Rasheed and Hyungjun Doh and Subramanian Chidambaram and Karthik Ramani},
    journal={arXiv},
    year={2024},
    eprint={2310.13167},
    abstract = {Mixed Reality (MR) is gaining prominence in manual task skill learning due to its in-situ, embodied, and immersive experience. To teach manual tasks, current methodologies reak the task into hierarchies (tasks into subtasks) and visualize not only the current subtasks but also the future ones that are causally related. We investigate the impact of visualizing causality within an MR framework on manual task skill learning. We conducted a user study with 48 participants, experimenting with how presenting tasks in hierarchical causality levels (no causality, event-level, interaction-level, and gesture-level causality) affects user comprehension and performance in a complex assembly task. The research finds that displaying all causality levels enhances user understanding and task execution, with a compromise of learning time. Based on the results, we further provide design recommendations and in-depth discussions for future manual task learning systems.},
    archivePrefix={arXiv},
    primaryClass={cs.HC},
    pdf={Visualizing_Causality.pdf},
    preview={Visualizing_Causality.png}
}